{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_jA-wQXLQBwL","executionInfo":{"status":"ok","timestamp":1719502971515,"user_tz":-330,"elapsed":27709,"user":{"displayName":"Samyaraj","userId":"09113023883148100370"}},"outputId":"1494dff3-557a-4981-ed0e-66adbd644b8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from os import listdir, mkdir\n","from glob import glob\n","from time import time\n","import cv2\n","import matplotlib.pyplot as plt\n","from imutils import rotate_bound\n","from os.path import isfile, exists\n","import argparse\n","from textwrap import dedent"],"metadata":{"id":"7IOpdbKqZOMs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def draw_table(data, classes, save_as=\"table\", save_to=\"images\"):\n","    \"\"\"\n","    Draw a table of precision and recall values and save it as an image.\n","\n","    :param data: Confusion matrix.\n","    :param classes: List of class names.\n","    :param save_as: Name of the file to save the table.\n","    :param save_to: Directory to save the table.\n","    \"\"\"\n","    result = [[precision(label, data), recall(label, data)] for label in range(len(classes))]\n","    columns = [\"precision\", \"recall\"]\n","    colors = plt.cm.BuPu(np.linspace(0, 0.5, len(classes)))[::-1]\n","    plt.table(colWidths=[.3, .3], cellText=result, cellLoc='center', rowLabels=classes, rowColours=colors, colLabels=columns, loc='center')\n","    plt.axis('off')\n","    plt.savefig(f'{save_to}/{save_as}.png')\n","    plt.show()"],"metadata":{"id":"m2msXJxaZtDr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def dataSetGenerator(path, resize=False, resize_to=224, percentage=100, dataAugmentation=False):\n","    \"\"\"\n","    Generate an image dataset from a directory of images.\n","\n","    :param str path: Path to the image dataset directory.\n","    :param bool resize: Whether to resize the images.\n","    :param int resize_to: Size to resize the images to.\n","    :param int percentage: Percentage of images to include in the dataset.\n","    :param bool dataAugmentation: Whether to apply data augmentation.\n","    :return: Tuple of images, labels, and classes.\n","    :rtype: tuple[np.ndarray, np.ndarray, np.ndarray]\n","    \"\"\"\n","    try:\n","        start_time = time()\n","        classes = listdir(path)\n","        image_list, labels = [], []\n","\n","        for classe in classes:\n","            for filename in glob(f'{path}/{classe}/*'):\n","                img = cv2.imread(filename, cv2.COLOR_BGR2RGB)\n","                if resize:\n","                    img = cv2.resize(img, (resize_to, resize_to))\n","                image_list.append(img)\n","                label = np.zeros(len(classes))\n","                label[classes.index(classe)] = 1\n","                labels.append(label)\n","\n","                if dataAugmentation:\n","                    for angle in np.arange(0, 360, 90):\n","                        rotated = rotate_bound(img, angle)\n","                        image_list.extend([rotated, np.fliplr(rotated)])\n","                        labels.extend([label, label])\n","\n","        indices = np.random.permutation(len(image_list))[:int(len(image_list) * percentage / 100)]\n","        print(f\"\\n --- dataSet generated in {np.round(time() - start_time)} seconds --- \\n\")\n","        return np.array([image_list[x] for x in indices]), np.array([labels[x] for x in indices]), np.array(classes)\n","\n","    except IOError as e:\n","        print(f\"I/O error({e.errno}): {e.strerror} \\nlike : {path}\")\n"],"metadata":{"id":"N34cM-DiZy1j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def dataSetToNPY(path, SaveTo=\"DataSets\", resize=True, resize_to=224, percentage=80, dataAugmentation=False):\n","    \"\"\"\n","    Generate a image dataSet from a picture dataSets and save it in pny files for fast reading in teast and train\n","\n","    the picture dataSets must be in the same structure to generate also labels\n","\n","    example of pictureFolder: http://weegee.vision.ucmerced.edu/datasets/landuse.html\n","\n","    picture dataSets\n","      |\n","      |----------class-1\n","      |        .   |-------image-1\n","      |        .   |         .\n","      |        .   |         .\n","      |        .   |         .\n","      |        .   |-------image-n\n","      |        .\n","      |-------class-n\n","\n","    :param str path: the path for picture dataSets folder (/)\n","    :param str SaveTo: the path when we save dataSets (/)\n","    :param bool resize: choose resize the pictures or not\n","    :param int resize_to: the new size of pictures\n","    :param int or bool dataAugmentation: apply data Augmentation Strategy\n","    :param int or float percentage: how many pictures you want to get from this pictureFolder for training\n","    :return: return dataset in npy files for fast Test and Train\n","    \"\"\"\n","    try:\n","        from os import mkdir\n","        from os.path import exists\n","        dataSet_name = path.replace('\\\\', \"/\").split(\"/\")[-1]\n","        mkdir(SaveTo + \"/\" + dataSet_name) if not exists(SaveTo + \"/\" + dataSet_name) else None\n","        SaveTo = SaveTo + \"/\" + dataSet_name\n","        data, labels, classes = dataSetGenerator(path, resize, resize_to, 100, dataAugmentation)\n","        indice = np.random.permutation(len(data))\n","        indice80 = indice[:int(len(data) * percentage / 100)]\n","        indice20 = indice[int(len(data) * percentage / 100):]\n","        np.save(SaveTo + \"/\" + dataSet_name + '_dataTrain.npy', [data[x] for x in indice80])\n","        np.save(SaveTo + \"/\" + dataSet_name + '_labelsTrain.npy', [labels[x] for x in indice80])\n","        np.save(SaveTo + \"/\" + dataSet_name + '_dataTest.npy', [data[x] for x in indice20])\n","        np.save(SaveTo + \"/\" + dataSet_name + '_labelsTest.npy', [labels[x] for x in indice20])\n","        np.save(SaveTo + \"/\" + dataSet_name + '_classes.npy', classes)\n","    except IOError as e:\n","        print(\"I/O error({}): {} \\nlike : {}\".format(e.errno, e.strerror, path))\n"],"metadata":{"id":"9nDWCO5jZ28L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls drive/MyDrive/Colab\\ Notebooks\\ 3/HamDet/output"],"metadata":{"id":"ECzvW9S-c_nu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def picShow(data, labels, classes, just=None, predict=None, autoClose=False, Save_as=\"pic\", save_to=\"images\"):\n","    \"\"\"\n","    Display images with their true and predicted classes.\n","\n","    :param data: List of images.\n","    :param labels: List of true labels for the images.\n","    :param classes: List of class names.\n","    :param just: Number of images to display.\n","    :param predict: List of predicted probabilities for each class.\n","    :param autoClose: Whether to automatically close the plot after displaying.\n","    :param Save_as: Name of the file to save the plot.\n","    :param save_to: Directory to save the plot.\n","    \"\"\"\n","    fig = plt.figure()\n","    if just is None:\n","        just = len(data)\n","\n","    for i in range(1, just + 1):\n","        true_out = classes[labels[i - 1].argmax()]\n","        sub = fig.add_subplot(np.rint(np.sqrt(just)), np.ceil(np.sqrt(just)), i)\n","        title = f\"true: {true_out}\"\n","        color = 'black'\n","\n","        if predict is not None:\n","            classIndex = predict[i - 1].argmax()\n","            predict_out = classes[classIndex]\n","            title += f\" predicted: {round(predict[i - 1][classIndex] * 100, 2)} {predict_out}\"\n","            color = 'green' if predict_out == true_out else 'red'\n","\n","        sub.set_title(title, color=color, fontsize=7, fontweight='bold')\n","        sub.axis('off')\n","        sub.imshow(data[i - 1], interpolation='nearest', aspect=\"auto\")\n","        plt.savefig(f'{save_to}/{Save_as}.png')\n","\n","    if autoClose:\n","        plt.show(0)\n","        plt.pause(10)\n","        plt.close()\n","    else:\n","        plt.show()"],"metadata":{"id":"B7UMk5t7Z57a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plotFiles(*paths, xlabel='# epochs', ylabel='Error and Accuracy', reduce_each=False, autoClose=False, ff=None, Save_as=\"plot\", save_to=\"images\"):\n","    \"\"\"\n","    Plot data from multiple files in one chart.\n","\n","    :param paths: Paths to the data files.\n","    :param xlabel: Label for the x-axis.\n","    :param ylabel: Label for the y-axis.\n","    :param reduce_each: Reduce the chart data by averaging every n points.\n","    :param autoClose: Whether to automatically close the plot after displaying.\n","    :param ff: Custom label for the plot.\n","    :param Save_as: Name of the file to save the plot.\n","    :param save_to: Directory to save the plot.\n","    \"\"\"\n","    for path in paths:\n","        if isfile(path):\n","            with open(path) as f:\n","                data = [float(i.strip('\\x00')) for i in f.read().split('\\n')[:-1] if i.strip('\\x00').isdigit()]\n","                resultat = []\n","\n","                if reduce_each:\n","                    for i in range(1, len(data), reduce_each):\n","                        l = data[i - 1:reduce_each * i]\n","                        resultat.append(sum(l) / float(len(l)))\n","                else:\n","                    resultat = data\n","\n","                label = ff if ff else path.replace(\"\\\\\", '/').split(\"/\")[-1].split(\".\")[0]\n","                plt.plot(resultat, label=label)\n","\n","        else:\n","            print(f\"I/O error({IOError.errno}): {IOError.strerror} \\nlike : {path}\")\n","\n","    plt.xlabel(xlabel)\n","    plt.ylabel(ylabel)\n","    plt.legend(loc='center left')\n","    plt.savefig(f'{save_to}/{Save_as}.png')\n","    if autoClose:\n","        plt.show(0)\n","        plt.pause(10)\n","        plt.close()\n","    else:\n","        plt.show()"],"metadata":{"id":"vcXKoPHnZ8Xj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plotSubFiles(*paths, xlabel='# epochs', ylabel='Error and Accuracy', reduce_each=False, autoClose=False):\n","    \"\"\"\n","    Plot data from multiple files in subplots.\n","\n","    :param paths: Paths to the data files.\n","    :param xlabel: Label for the x-axis.\n","    :param ylabel: Label for the y-axis.\n","    :param reduce_each: Reduce the chart data by averaging every n points.\n","    :param autoClose: Whether to automatically close the plot after displaying.\n","    \"\"\"\n","    fig = plt.figure()\n","    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n","    sub_index = 1\n","\n","    for path in paths:\n","        if isfile(path):\n","            with open(path) as f:\n","                data = [float(i.strip('\\x00')) for i in f.read().split('\\n')[:-1] if i.strip('\\x00').isdigit()]\n","                resultat = []\n","\n","                if reduce_each:\n","                    for i in range(1, len(data), reduce_each):\n","                        l = data[i - 1:reduce_each * i]\n","                        resultat.append(sum(l) / float(len(l)))\n","                else:\n","                    resultat = data\n","\n","                sub = fig.add_subplot(np.rint(np.sqrt(len(paths))), np.ceil(np.sqrt(len(paths))), sub_index)\n","                sub_index += 1\n","                sub.set_title(path.replace(\"\\\\\", '/').split(\"/\")[-1].split(\".\")[0], fontsize=7, fontweight='bold')\n","                sub.plot(resultat)\n","\n","        else:\n","            print(f\"I/O error({IOError.errno}): {IOError.strerror} \\nlike : {path}\")\n","\n","    plt.xlabel(xlabel)\n","    plt.ylabel(ylabel)\n","    if autoClose:\n","        plt.show(0)\n","        plt.pause(10)\n","        plt.close()\n","    else:\n","        plt.show()"],"metadata":{"id":"gMb2ubiAaAEA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_histories(histories, keys=[\"loss\", \"val_loss\"], legend=[\"Train\", \"Validation\"], xlabel='# epochs', ylabel='Error', autoClose=False, Save_as=\"plot\", save_to=\"images\"):\n","    \"\"\"\n","    Plot training histories.\n","\n","    :param histories: List of history objects.\n","    :param keys: Keys to plot from the history objects.\n","    :param legend: Legend for the plot.\n","    :param xlabel: Label for the x-axis.\n","    :param ylabel: Label for the y-axis.\n","    :param autoClose: Whether to automatically close the plot after displaying.\n","    :param Save_as: Name of the file to save the plot.\n","    :param save_to: Directory to save the plot.\n","    \"\"\"\n","    for history in histories:\n","        for key in keys:\n","            plt.plot(history.history[key])\n","\n","    plt.xlabel(xlabel)\n","    plt.ylabel(ylabel)\n","    plt.legend(legend, loc='upper left')\n","    plt.savefig(f'{save_to}/{Save_as}.png')\n","    if autoClose:\n","        plt.show(0)\n","        plt.pause(10)\n","        plt.close()\n","    else:\n","        plt.show()"],"metadata":{"id":"qaZ5-to9aOS0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def precision(label, confusion_matrix):\n","    \"\"\"\n","    Calculate the precision for a given class label.\n","\n","    :param int label: Class label.\n","    :param np.ndarray confusion_matrix: Confusion matrix.\n","    :return: Precision value.\n","    :rtype: float\n","    \"\"\"\n","    col = confusion_matrix[:, label]\n","    return confusion_matrix[label, label] / col.sum()"],"metadata":{"id":"ZQWKLvtCaRQs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def recall(label, confusion_matrix):\n","    \"\"\"\n","    Calculate the recall for a given class label.\n","\n","    :param int label: Class label.\n","    :param np.ndarray confusion_matrix: Confusion matrix.\n","    :return: Recall value.\n","    :rtype: float\n","    \"\"\"\n","    row = confusion_matrix[label, :]\n","    return confusion_matrix[label, label] / row.sum()"],"metadata":{"id":"8kJmHKj9aTW7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def accuracy(confusion_matrix):\n","    \"\"\"\n","    Calculate the accuracy from the confusion matrix.\n","\n","    :param np.ndarray confusion_matrix: Confusion matrix.\n","    :return: Accuracy value.\n","    :rtype: float\n","    \"\"\"\n","    diagonal_sum = confusion_matrix.trace()\n","    sum_of_all_elements = confusion_matrix.sum()\n","    return diagonal_sum / sum_of_all_elements"],"metadata":{"id":"H21RYqS7aVrV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def draw_confusion_matrix(confusion_matrix, class_labels, Save_as=\"Confusion Matrix\", save_to=\"images\"):\n","    \"\"\"\n","    Draw and save the confusion matrix.\n","\n","    :param np.ndarray confusion_matrix: Confusion matrix.\n","    :param list class_labels: List of class labels.\n","    :param Save_as: Name of the file to save the confusion matrix.\n","    :param save_to: Directory to save the confusion matrix.\n","    \"\"\"\n","    num_classes = len(class_labels)\n","    fig, ax = plt.subplots()\n","    cax = ax.matshow(confusion_matrix, cmap=plt.cm.Blues)\n","    fig.colorbar(cax)\n","\n","    for i in range(num_classes):\n","        for j in range(num_classes):\n","            ax.text(i, j, str(confusion_matrix[j, i]), va='center', ha='center')\n","\n","    ax.set_xticks(np.arange(num_classes))\n","    ax.set_xticklabels(class_labels)\n","    ax.set_yticks(np.arange(num_classes))\n","    ax.set_yticklabels(class_labels)\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.savefig(f'{save_to}/{Save_as}.png')\n","    plt.show()"],"metadata":{"id":"47l3VBU7aXT0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","\n","    path = 'drive/MyDrive/Colab Notebooks 3/HamDet/ham'\n","    SaveTo = 'drive/MyDrive/Colab Notebooks 3/HamDet/output'\n","    resize = True\n","    resize_to = 224\n","    percentage = 80\n","    dataAugmentation = False\n","\n","    # Print the paths to verify\n","    print(f\"Dataset path: {path}\")\n","    print(f\"Save to: {SaveTo}\")\n","\n","    dataSetToNPY(path, SaveTo, resize, resize_to, percentage, dataAugmentation)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Af3HKIedaZnj","executionInfo":{"status":"ok","timestamp":1719502984550,"user_tz":-330,"elapsed":9823,"user":{"displayName":"Samyaraj","userId":"09113023883148100370"}},"outputId":"765359ae-4f97-416d-c9c0-575cddf11851"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset path: drive/MyDrive/Colab Notebooks 3/HamDet/ham\n","Save to: drive/MyDrive/Colab Notebooks 3/HamDet/output\n","\n"," --- dataSet generated in 6.0 seconds --- \n","\n"]}]},{"cell_type":"code","source":["classes_name = \"ham\"  # Set the dataset name here\n","batch_size = 10  # Set the batch size here\n","epochs = 30  # Set the number of epochs here\n","\n","classes = np.load(f\"drive/MyDrive/Colab Notebooks 3/HamDet/output/ham/{classes_name}_classes.npy\")\n","batch = np.load(f\"drive/MyDrive/Colab Notebooks 3/HamDet/output/ham/{classes_name}_dataTrain.npy\")\n","labels = np.load(f\"drive/MyDrive/Colab Notebooks 3/HamDet/output/ham/{classes_name}_labelsTrain.npy\")\n","\n","classes_num = len(classes)\n","rib = batch.shape[1]  # picture Rib\n","\n","with tf.device('/cpu:0'):\n","    with tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=int(environ['NUMBER_OF_PROCESSORS']))) as sess:\n","        images = tf.placeholder(tf.float32, [None, rib, rib, 3])\n","        true_out = tf.placeholder(tf.float32, [None, classes_num])\n","        train_mode = tf.placeholder(tf.bool)\n","\n","        try:\n","            vgg = vgg16.Vgg16(f'drive/MyDrive/Colab Notebooks 3/HamDet/output/output/VGG16_{classes_name}.npy', classes_num)\n","        except:\n","            print(f'drive/MyDrive/Colab Notebooks 3/HamDet/output/outputVGG16_{classes_name}.npy Not Exist')\n","            vgg = vgg16.Vgg16(None, classes_num)\n","        vgg.build(images, train_mode)\n","\n","        # print number of variables used: 143667240 variables, i.e. ideal size = 548MB\n","        print('number of variables used:', vgg.get_var_count())\n","\n","        sess.run(tf.global_variables_initializer())\n","\n","        # test classification\n","        prob = sess.run(vgg.prob, feed_dict={images: batch[:10], train_mode: False})\n","        picShow(batch[:10], labels[:10], classes, None, prob, True)\n","\n","        # simple 1-step training\n","        cost = tf.reduce_sum((vgg.prob - true_out) ** 2)\n","        train = tf.train.GradientDescentOptimizer(0.0001).minimize(cost)\n","\n","        correct_prediction = tf.equal(tf.argmax(prob, 1), tf.argmax(true_out, 1))\n","        acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","        batche_num = batch.shape[0]\n","        costs = []\n","        accs = []\n","        for epoch in range(epochs):\n","            indice = np.random.permutation(batche_num)\n","            counter = 0\n","            for i in range(int(batche_num / batch_size)):\n","                min_batch = indice[i * batch_size:(i + 1) * batch_size]\n","                cur_cost, _, cur_acc = sess.run([cost, train, acc], feed_dict={images: batch[min_batch], true_out: labels[min_batch], train_mode: True})\n","                print(f\"Epoch: {epoch} Batch: {i} Loss: {cur_cost}\")\n","                accs.append(cur_acc)\n","                costs.append(cur_cost)\n","                counter += 1\n","                if counter % 100 == 0:\n","                    #  save graph data\n","                    append(costs, f'Data/COST16_{classes_name}.txt')\n","                    append(accs, f'Data/ACC16_{classes_name}.txt')\n","                    # save Weights\n","                    vgg.save_npy(sess, f'Weights/VGG16_{classes_name}.npy')\n","\n","            #  save graph data\n","            append(costs, f'Data/COST16_{classes_name}.txt')\n","            append(accs, f'Data/ACC16_{classes_name}.txt')\n","            #  save Weights\n","            vgg.save_npy(sess, f'Weights/VGG16_{classes_name}.npy')\n","\n","        # test classification again, should have a higher probability about tiger\n","        prob = sess.run(vgg.prob, feed_dict={images: batch[:10], train_mode: False})\n","        picShow(batch[:10], labels[:10], classes, None, prob)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"oQFoDWsujfxm","executionInfo":{"status":"error","timestamp":1719502990087,"user_tz":-330,"elapsed":498,"user":{"displayName":"Samyaraj","userId":"09113023883148100370"}},"outputId":"836d9674-d7fc-49e1-e04c-c9f57167e5d0"},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"tuple index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-e4b79e2017a9>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mclasses_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# picture Rib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: tuple index out of range"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Load data from a .npy file\n","data = np.load(f\"drive/MyDrive/Colab Notebooks 3/HamDet/output/ham/{classes_name}_classes.npy\")\n","\n","# Now you can use the 'data' variable to access the contents of the .npy file\n","print(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3FVTIihPk7lo","executionInfo":{"status":"ok","timestamp":1719500109990,"user_tz":-330,"elapsed":672,"user":{"displayName":"Samyaraj","userId":"09113023883148100370"}},"outputId":"63492cbc-fe17-48ce-b48e-78c424a0c818"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Archery (67).jpg' 'Archery (5).jpg' 'Archery (100).jpg' ...\n"," 'Archery (122).jpg' 'Archery (53).jpg' 'Archery (78).jpg']\n"]}]},{"cell_type":"code","source":["#Restnet 152\n","\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Install necessary packages (if not already installed)\n","!pip install tensorflow\n","\n","from tensorflow.keras.applications import ResNet152\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import Adam\n","\n","# Define image data generators for training and validation\n","train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n","val_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Define image paths for training and validation data (update with your directory structure)\n","train_dir = \"/content/drive/My Drive/Research/Paper2/Anaconda1/Train\"\n","val_dir = \"/content/drive/My Drive/Research/Paper2/Anaconda1/Test\"\n","\n","# Set image dimensions\n","img_width, img_height = 224, 224\n","\n","# Create training and validation data generators\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(img_width, img_height),\n","    batch_size=32,\n","    class_mode='categorical'  # Adjust for your classification task\n",")\n","val_generator = val_datagen.flow_from_directory(\n","    val_dir,\n","    target_size=(img_width, img_height),\n","    batch_size=8,\n","    class_mode='categorical'  # Adjust for your classification task\n",")\n","\n","# Load the pre-trained ResNet152 model without the top layer\n","base_model = ResNet152(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n","\n","# Freeze the pre-trained layers for fine-tuning\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# Add custom layers for classification\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024, activation='relu')(x)  # Adjust number of units based on your dataset\n","predictions = Dense(2, activation='softmax')(x)  # Adjust output layer for your number of classes\n","\n","# Create the final model\n","model = Model(inputs=base_model.input, outputs=predictions)\n","\n","# Compile the model\n","model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model on the training data\n","hist = model.fit(train_generator, epochs=100, validation_data=val_generator)  # Adjust epochs as needed\n"],"metadata":{"id":"hFT8ID65lC_b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Install necessary packages (if not already installed)\n","!pip install tensorflow\n","\n","from tensorflow.keras.applications import ResNet152\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import Adam\n","\n","# Define image data generators for training and validation\n","train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n","val_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Define image paths for training and validation data (update with your directory structure)\n","train_dir = \"/content/drive/My Drive/Research/Paper2/Anaconda1/Train\"\n","val_dir = \"/content/drive/My Drive/Research/Paper2/Anaconda1/Test\"\n","\n","# Set image dimensions\n","img_width, img_height = 224, 224\n","\n","# Create training and validation data generators\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(img_width, img_height),\n","    batch_size=32,\n","    class_mode='categorical'  # Adjust for your classification task\n",")\n","val_generator = val_datagen.flow_from_directory(\n","    val_dir,\n","    target_size=(img_width, img_height),\n","    batch_size=8,\n","    class_mode='categorical'  # Adjust for your classification task\n",")\n","\n","# Load the pre-trained ResNet152 model without the top layer\n","base_model = ResNet152(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n","\n","# Freeze the pre-trained layers for fine-tuning\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# Add custom layers for classification\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024, activation='relu')(x)  # Adjust number of units based on your dataset\n","predictions = Dense(2, activation='softmax')(x)  # Adjust output layer for your number of classes\n","\n","# Create the final model\n","model = Model(inputs=base_model.input, outputs=predictions)\n","\n","# Compile the model\n","model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model on the training data\n","hist = model.fit(train_generator, epochs=100, validation_data=val_generator)  # Adjust epochs as needed\n"],"metadata":{"id":"N3A-PLTKjf5G"},"execution_count":null,"outputs":[]}]}